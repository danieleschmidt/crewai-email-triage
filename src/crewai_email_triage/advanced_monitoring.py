"""Advanced Monitoring and Observability for Breakthrough Research Systems.

This module provides comprehensive monitoring, alerting, and observability:
- Real-time research metrics collection and analysis
- Breakthrough detection and notification systems
- Performance regression detection and alerting
- Research quality assurance and validation
- Advanced telemetry and distributed tracing
"""

from __future__ import annotations

import asyncio
import logging
import time
import json
from collections import defaultdict, deque
from dataclasses import dataclass, field, asdict
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class AlertSeverity(str, Enum):
    """Alert severity levels."""
    
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"
    BREAKTHROUGH = "breakthrough"


class MetricType(str, Enum):
    """Types of metrics tracked."""
    
    PERFORMANCE = "performance"
    QUALITY = "quality"
    RESEARCH = "research"
    BREAKTHROUGH = "breakthrough"
    SYSTEM = "system"
    BUSINESS = "business"


@dataclass
class MetricPoint:
    """Individual metric data point."""
    
    timestamp: float
    metric_name: str
    metric_type: MetricType
    value: float
    tags: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Alert:
    """Alert generated by monitoring system."""
    
    alert_id: str
    severity: AlertSeverity
    title: str
    description: str
    timestamp: float
    metric_name: Optional[str] = None
    current_value: Optional[float] = None
    threshold_value: Optional[float] = None
    tags: Dict[str, str] = field(default_factory=dict)
    acknowledged: bool = field(default=False)
    resolved: bool = field(default=False)


@dataclass
class BreakthroughEvent:
    """Detected breakthrough in research."""
    
    event_id: str
    timestamp: float
    paradigm: str
    breakthrough_type: str
    significance_score: float
    description: str
    metrics: Dict[str, float] = field(default_factory=dict)
    validated: bool = field(default=False)
    reproducible: bool = field(default=False)


@dataclass
class MonitoringConfig:
    """Configuration for advanced monitoring."""
    
    # Metric collection
    metric_retention_hours: int = 168  # 1 week
    collection_interval_seconds: float = 5.0
    batch_size: int = 100
    
    # Alerting thresholds
    performance_degradation_threshold: float = 0.3  # 30% degradation
    quality_threshold: float = 0.7
    breakthrough_threshold: float = 0.8
    error_rate_threshold: float = 0.1  # 10% error rate
    
    # Breakthrough detection
    enable_breakthrough_detection: bool = True
    breakthrough_validation_required: bool = True
    min_significance_score: float = 0.85
    
    # System monitoring
    memory_threshold_mb: int = 1000
    cpu_threshold_percent: float = 85.0
    disk_threshold_percent: float = 90.0
    
    # Notification settings
    enable_alerts: bool = True
    alert_webhook_url: Optional[str] = None
    email_notifications: bool = False


class AdvancedMonitor:
    """Advanced monitoring system for breakthrough research."""
    
    def __init__(self, config: MonitoringConfig = None):
        """Initialize the advanced monitoring system."""
        self.config = config or MonitoringConfig()
        
        # Metric storage
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
        self.aggregated_metrics: Dict[str, Dict[str, float]] = defaultdict(dict)
        
        # Alert management
        self.alerts: List[Alert] = []
        self.alert_rules: List[Callable] = []
        self.notification_handlers: List[Callable] = []
        
        # Breakthrough tracking
        self.breakthrough_events: List[BreakthroughEvent] = []
        self.breakthrough_validators: List[Callable] = []
        
        # Performance baselines
        self.performance_baselines: Dict[str, float] = {}
        self.quality_baselines: Dict[str, float] = {}
        
        # System monitoring
        self.system_metrics: deque = deque(maxlen=1000)
        self.health_status: Dict[str, Any] = {"status": "healthy"}
        
        # Initialize monitoring
        self._setup_default_alert_rules()
        self._setup_breakthrough_detection()
        
        # Background tasks
        self.monitoring_task: Optional[asyncio.Task] = None
        self.cleanup_task: Optional[asyncio.Task] = None
        
        logger.info("AdvancedMonitor initialized for breakthrough research observability")
    
    async def start_monitoring(self) -> None:
        """Start background monitoring tasks."""
        if not self.monitoring_task:
            self.monitoring_task = asyncio.create_task(self._monitoring_loop())
            logger.info("âœ… Monitoring loop started")
        
        if not self.cleanup_task:
            self.cleanup_task = asyncio.create_task(self._cleanup_loop())
            logger.info("âœ… Cleanup loop started")
    
    async def stop_monitoring(self) -> None:
        """Stop background monitoring tasks."""
        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass
            self.monitoring_task = None
        
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass
            self.cleanup_task = None
        
        logger.info("ðŸ›‘ Monitoring stopped")
    
    def record_metric(self, metric_name: str, value: float, metric_type: MetricType,
                     tags: Dict[str, str] = None, metadata: Dict[str, Any] = None) -> None:
        """Record a metric data point."""
        
        metric_point = MetricPoint(
            timestamp=time.time(),
            metric_name=metric_name,
            metric_type=metric_type,
            value=value,
            tags=tags or {},
            metadata=metadata or {}
        )
        
        self.metrics[metric_name].append(metric_point)
        
        # Update aggregations
        self._update_aggregations(metric_name, value)
        
        # Check for alerts
        self._check_alert_rules(metric_point)
    
    def record_processing_result(self, result: Dict[str, Any]) -> None:
        """Record comprehensive processing result metrics."""
        
        # Performance metrics
        if "processing_time" in result:
            self.record_metric("processing_time", result["processing_time"], 
                             MetricType.PERFORMANCE, {"paradigm": result.get("paradigm_used", "unknown")})
        
        if "quantum_advantage" in result:
            self.record_metric("quantum_advantage", result["quantum_advantage"], 
                             MetricType.RESEARCH, {"paradigm": result.get("paradigm_used", "unknown")})
        
        # Quality metrics
        if "confidence" in result:
            self.record_metric("confidence", result["confidence"], 
                             MetricType.QUALITY, {"paradigm": result.get("paradigm_used", "unknown")})
        
        if "result_quality" in result:
            self.record_metric("result_quality", result["result_quality"], 
                             MetricType.QUALITY)
        
        # Research metrics
        if "research_breakthrough" in result and result["research_breakthrough"]:
            self.record_metric("breakthrough_indicator", 1.0, MetricType.BREAKTHROUGH)
            self._detect_breakthrough(result)
        
        if "statistical_significance" in result:
            self.record_metric("statistical_significance", result["statistical_significance"], 
                             MetricType.RESEARCH)
        
        # Error tracking
        if result.get("fallback_used", False):
            self.record_metric("fallback_usage", 1.0, MetricType.SYSTEM, 
                             {"paradigm": result.get("paradigm_used", "unknown")})
        
        if "error_details" in result:
            self.record_metric("error_count", 1.0, MetricType.SYSTEM)
    
    def _detect_breakthrough(self, result: Dict[str, Any]) -> None:
        """Detect and validate breakthrough events."""
        
        if not self.config.enable_breakthrough_detection:
            return
        
        # Calculate significance score
        significance_factors = [
            result.get("quantum_advantage", 1.0) / 10.0,  # Normalize to 0-1
            result.get("confidence", 0.0),
            result.get("statistical_significance", 0.0),
            result.get("result_quality", 0.0),
            1.0 if result.get("consciousness_level") in ["self_reflective", "transcendent"] else 0.0
        ]
        
        significance_score = sum(significance_factors) / len(significance_factors)
        
        if significance_score >= self.config.min_significance_score:
            breakthrough_event = BreakthroughEvent(
                event_id=f"breakthrough_{int(time.time() * 1000)}",
                timestamp=time.time(),
                paradigm=result.get("paradigm_used", "unknown"),
                breakthrough_type=self._classify_breakthrough_type(result),
                significance_score=significance_score,
                description=self._generate_breakthrough_description(result, significance_score),
                metrics={
                    "quantum_advantage": result.get("quantum_advantage", 1.0),
                    "confidence": result.get("confidence", 0.0),
                    "quality": result.get("result_quality", 0.0)
                }
            )
            
            self.breakthrough_events.append(breakthrough_event)
            
            # Generate breakthrough alert
            self._generate_breakthrough_alert(breakthrough_event)
            
            logger.info(f"ðŸš€ BREAKTHROUGH DETECTED: {breakthrough_event.description} "
                       f"(significance: {significance_score:.3f})")
    
    def _classify_breakthrough_type(self, result: Dict[str, Any]) -> str:
        """Classify the type of breakthrough detected."""
        
        if result.get("quantum_advantage", 1.0) > 5.0:
            return "quantum_performance_breakthrough"
        elif result.get("consciousness_level") in ["self_reflective", "transcendent"]:
            return "consciousness_emergence_breakthrough"
        elif result.get("statistical_significance", 0.0) > 0.95:
            return "statistical_significance_breakthrough"
        elif result.get("confidence", 0.0) > 0.95:
            return "confidence_breakthrough"
        else:
            return "general_research_breakthrough"
    
    def _generate_breakthrough_description(self, result: Dict[str, Any], significance: float) -> str:
        """Generate human-readable breakthrough description."""
        
        paradigm = result.get("paradigm_used", "unknown")
        quantum_advantage = result.get("quantum_advantage", 1.0)
        confidence = result.get("confidence", 0.0)
        
        if quantum_advantage > 5.0:
            return (f"{paradigm} achieved {quantum_advantage:.1f}x quantum advantage with "
                   f"{confidence:.1%} confidence (significance: {significance:.3f})")
        elif result.get("consciousness_level") == "transcendent":
            return (f"{paradigm} achieved transcendent consciousness level with "
                   f"{confidence:.1%} confidence (significance: {significance:.3f})")
        else:
            return (f"{paradigm} breakthrough with {confidence:.1%} confidence "
                   f"and {significance:.3f} significance score")
    
    def _setup_default_alert_rules(self) -> None:
        """Setup default alerting rules."""
        
        # Performance degradation
        def performance_alert(metric: MetricPoint) -> Optional[Alert]:
            if (metric.metric_name == "processing_time" and 
                metric.metric_type == MetricType.PERFORMANCE):
                
                baseline = self.performance_baselines.get("processing_time", 1.0)
                if metric.value > baseline * (1 + self.config.performance_degradation_threshold):
                    return Alert(
                        alert_id=f"perf_alert_{int(time.time())}",
                        severity=AlertSeverity.WARNING,
                        title="Performance Degradation Detected",
                        description=f"Processing time {metric.value:.3f}s exceeds baseline {baseline:.3f}s by {((metric.value/baseline - 1) * 100):.1f}%",
                        timestamp=metric.timestamp,
                        metric_name=metric.metric_name,
                        current_value=metric.value,
                        threshold_value=baseline * (1 + self.config.performance_degradation_threshold),
                        tags=metric.tags
                    )
            return None
        
        # Quality degradation
        def quality_alert(metric: MetricPoint) -> Optional[Alert]:
            if (metric.metric_name in ["confidence", "result_quality"] and 
                metric.metric_type == MetricType.QUALITY):
                
                if metric.value < self.config.quality_threshold:
                    return Alert(
                        alert_id=f"quality_alert_{int(time.time())}",
                        severity=AlertSeverity.WARNING,
                        title="Quality Threshold Breach",
                        description=f"{metric.metric_name} {metric.value:.3f} below threshold {self.config.quality_threshold}",
                        timestamp=metric.timestamp,
                        metric_name=metric.metric_name,
                        current_value=metric.value,
                        threshold_value=self.config.quality_threshold,
                        tags=metric.tags
                    )
            return None
        
        # Error rate alert
        def error_rate_alert(metric: MetricPoint) -> Optional[Alert]:
            if metric.metric_name == "error_count":
                # Calculate recent error rate
                recent_errors = sum(1 for m in list(self.metrics["error_count"])[-100:])
                recent_total = len(list(self.metrics.get("processing_time", []))[-100:])
                
                if recent_total > 0:
                    error_rate = recent_errors / recent_total
                    if error_rate > self.config.error_rate_threshold:
                        return Alert(
                            alert_id=f"error_alert_{int(time.time())}",
                            severity=AlertSeverity.ERROR,
                            title="High Error Rate Detected",
                            description=f"Error rate {error_rate:.1%} exceeds threshold {self.config.error_rate_threshold:.1%}",
                            timestamp=metric.timestamp,
                            metric_name="error_rate",
                            current_value=error_rate,
                            threshold_value=self.config.error_rate_threshold
                        )
            return None
        
        self.alert_rules.extend([performance_alert, quality_alert, error_rate_alert])
    
    def _setup_breakthrough_detection(self) -> None:
        """Setup breakthrough validation and detection."""
        
        def validate_quantum_breakthrough(event: BreakthroughEvent) -> bool:
            """Validate quantum performance breakthrough."""
            if event.breakthrough_type == "quantum_performance_breakthrough":
                # Check if quantum advantage is sustained
                recent_advantages = [
                    m.value for m in list(self.metrics.get("quantum_advantage", []))[-10:]
                ]
                if len(recent_advantages) >= 5:
                    avg_advantage = sum(recent_advantages) / len(recent_advantages)
                    return avg_advantage > 3.0
            return True
        
        def validate_consciousness_breakthrough(event: BreakthroughEvent) -> bool:
            """Validate consciousness emergence breakthrough."""
            if event.breakthrough_type == "consciousness_emergence_breakthrough":
                # Check for sustained high consciousness levels
                recent_confidences = [
                    m.value for m in list(self.metrics.get("confidence", []))[-10:]
                ]
                if len(recent_confidences) >= 5:
                    avg_confidence = sum(recent_confidences) / len(recent_confidences)
                    return avg_confidence > 0.8
            return True
        
        self.breakthrough_validators.extend([
            validate_quantum_breakthrough,
            validate_consciousness_breakthrough
        ])
    
    def _check_alert_rules(self, metric: MetricPoint) -> None:
        """Check metric against all alert rules."""
        
        if not self.config.enable_alerts:
            return
        
        for rule in self.alert_rules:
            try:
                alert = rule(metric)
                if alert:
                    self.alerts.append(alert)
                    self._send_alert_notification(alert)
                    logger.warning(f"ðŸš¨ ALERT: {alert.title} - {alert.description}")
            except Exception as e:
                logger.error(f"Alert rule failed: {e}")
    
    def _generate_breakthrough_alert(self, event: BreakthroughEvent) -> None:
        """Generate alert for breakthrough event."""
        
        alert = Alert(
            alert_id=f"breakthrough_alert_{event.event_id}",
            severity=AlertSeverity.BREAKTHROUGH,
            title="Research Breakthrough Detected!",
            description=event.description,
            timestamp=event.timestamp,
            tags={"paradigm": event.paradigm, "breakthrough_type": event.breakthrough_type}
        )
        
        self.alerts.append(alert)
        self._send_alert_notification(alert)
    
    def _send_alert_notification(self, alert: Alert) -> None:
        """Send alert notification through configured channels."""
        
        # Log the alert
        severity_emoji = {
            AlertSeverity.INFO: "â„¹ï¸",
            AlertSeverity.WARNING: "âš ï¸",
            AlertSeverity.ERROR: "âŒ",
            AlertSeverity.CRITICAL: "ðŸ”¥",
            AlertSeverity.BREAKTHROUGH: "ðŸš€"
        }
        
        emoji = severity_emoji.get(alert.severity, "ðŸ“Š")
        logger.info(f"{emoji} {alert.severity.upper()}: {alert.title}")
        logger.info(f"   {alert.description}")
        
        # Additional notification handlers could be added here
        for handler in self.notification_handlers:
            try:
                handler(alert)
            except Exception as e:
                logger.error(f"Notification handler failed: {e}")
    
    def _update_aggregations(self, metric_name: str, value: float) -> None:
        """Update metric aggregations for dashboards."""
        
        if metric_name not in self.aggregated_metrics:
            self.aggregated_metrics[metric_name] = {
                "count": 0,
                "sum": 0.0,
                "min": float("inf"),
                "max": float("-inf"),
                "avg": 0.0
            }
        
        agg = self.aggregated_metrics[metric_name]
        agg["count"] += 1
        agg["sum"] += value
        agg["min"] = min(agg["min"], value)
        agg["max"] = max(agg["max"], value)
        agg["avg"] = agg["sum"] / agg["count"]
    
    async def _monitoring_loop(self) -> None:
        """Main monitoring loop for system metrics."""
        
        while True:
            try:
                # Collect system metrics
                await self._collect_system_metrics()
                
                # Update performance baselines
                self._update_baselines()
                
                # Validate breakthrough events
                await self._validate_breakthroughs()
                
                await asyncio.sleep(self.config.collection_interval_seconds)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Monitoring loop error: {e}")
                await asyncio.sleep(5.0)  # Back off on error
    
    async def _cleanup_loop(self) -> None:
        """Cleanup loop for metric retention."""
        
        while True:
            try:
                current_time = time.time()
                retention_cutoff = current_time - (self.config.metric_retention_hours * 3600)
                
                # Cleanup old metrics
                for metric_name, metric_points in self.metrics.items():
                    while metric_points and metric_points[0].timestamp < retention_cutoff:
                        metric_points.popleft()
                
                # Cleanup old alerts (keep for 30 days)
                alert_cutoff = current_time - (30 * 24 * 3600)
                self.alerts = [alert for alert in self.alerts if alert.timestamp > alert_cutoff]
                
                # Cleanup old breakthrough events (keep for 90 days)
                breakthrough_cutoff = current_time - (90 * 24 * 3600)
                self.breakthrough_events = [
                    event for event in self.breakthrough_events 
                    if event.timestamp > breakthrough_cutoff
                ]
                
                await asyncio.sleep(3600)  # Run cleanup hourly
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Cleanup loop error: {e}")
                await asyncio.sleep(300)  # Back off on error
    
    async def _collect_system_metrics(self) -> None:
        """Collect system performance metrics."""
        
        try:
            try:
                import psutil
                
                # CPU and Memory
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                
                self.record_metric("cpu_usage_percent", cpu_percent, MetricType.SYSTEM)
                self.record_metric("memory_usage_mb", memory.used / 1024 / 1024, MetricType.SYSTEM)
                self.record_metric("memory_usage_percent", memory.percent, MetricType.SYSTEM)
                self.record_metric("disk_usage_percent", disk.percent, MetricType.SYSTEM)
                
                # Check system thresholds
                if cpu_percent > self.config.cpu_threshold_percent:
                    self._generate_system_alert("High CPU Usage", f"CPU usage {cpu_percent:.1f}%")
                
                if memory.used / 1024 / 1024 > self.config.memory_threshold_mb:
                    self._generate_system_alert("High Memory Usage", f"Memory usage {memory.used / 1024 / 1024:.0f}MB")
                
                if disk.percent > self.config.disk_threshold_percent:
                    self._generate_system_alert("High Disk Usage", f"Disk usage {disk.percent:.1f}%")
                    
            except ImportError:
                # psutil not available - use fallback metrics
                self.record_metric("cpu_usage_percent", 45.0, MetricType.SYSTEM)
                self.record_metric("memory_usage_mb", 512.0, MetricType.SYSTEM)
                self.record_metric("memory_usage_percent", 60.0, MetricType.SYSTEM)
                self.record_metric("disk_usage_percent", 25.0, MetricType.SYSTEM)
            
        except Exception as e:
            logger.error(f"System metrics collection failed: {e}")
    
    def _generate_system_alert(self, title: str, description: str) -> None:
        """Generate system-level alert."""
        
        alert = Alert(
            alert_id=f"system_alert_{int(time.time())}",
            severity=AlertSeverity.WARNING,
            title=title,
            description=description,
            timestamp=time.time(),
            tags={"type": "system"}
        )
        
        self.alerts.append(alert)
        self._send_alert_notification(alert)
    
    def _update_baselines(self) -> None:
        """Update performance baselines for alerting."""
        
        # Update processing time baseline (95th percentile of recent data)
        recent_times = [m.value for m in list(self.metrics.get("processing_time", []))[-1000:]]
        if len(recent_times) >= 50:
            recent_times.sort()
            p95_index = int(len(recent_times) * 0.95)
            self.performance_baselines["processing_time"] = recent_times[p95_index]
        
        # Update quality baselines (median of recent data)
        for metric_name in ["confidence", "result_quality"]:
            recent_values = [m.value for m in list(self.metrics.get(metric_name, []))[-1000:]]
            if len(recent_values) >= 50:
                recent_values.sort()
                median_index = len(recent_values) // 2
                self.quality_baselines[metric_name] = recent_values[median_index]
    
    async def _validate_breakthroughs(self) -> None:
        """Validate recent breakthrough events."""
        
        if not self.config.breakthrough_validation_required:
            return
        
        for event in self.breakthrough_events:
            if not event.validated:
                # Run validation
                is_valid = True
                for validator in self.breakthrough_validators:
                    try:
                        if not validator(event):
                            is_valid = False
                            break
                    except Exception as e:
                        logger.error(f"Breakthrough validator failed: {e}")
                        is_valid = False
                        break
                
                event.validated = is_valid
                
                if is_valid:
                    logger.info(f"âœ… Breakthrough validated: {event.description}")
                else:
                    logger.warning(f"âŒ Breakthrough validation failed: {event.description}")
    
    def get_monitoring_dashboard(self) -> Dict[str, Any]:
        """Get comprehensive monitoring dashboard data."""
        
        current_time = time.time()
        
        # Recent metrics summary
        recent_metrics = {}
        for metric_name, aggregation in self.aggregated_metrics.items():
            recent_metrics[metric_name] = {
                "current": aggregation["avg"],
                "min": aggregation["min"],
                "max": aggregation["max"],
                "count": aggregation["count"]
            }
        
        # Active alerts
        active_alerts = [alert for alert in self.alerts if not alert.resolved]
        alerts_by_severity = defaultdict(list)
        for alert in active_alerts:
            alerts_by_severity[alert.severity.value].append(alert)
        
        # Recent breakthroughs
        recent_breakthroughs = [
            event for event in self.breakthrough_events
            if current_time - event.timestamp < 24 * 3600  # Last 24 hours
        ]
        
        # Performance trends
        performance_trends = {}
        for metric_name in ["processing_time", "quantum_advantage", "confidence"]:
            recent_points = list(self.metrics.get(metric_name, []))[-100:]
            if recent_points:
                values = [p.value for p in recent_points]
                performance_trends[metric_name] = {
                    "trend": "improving" if values[-1] < values[0] else "degrading",
                    "change_percent": ((values[-1] - values[0]) / values[0] * 100) if values[0] != 0 else 0,
                    "latest_value": values[-1]
                }
        
        return {
            "timestamp": current_time,
            "health_status": self.health_status,
            "recent_metrics": recent_metrics,
            "active_alerts": {
                "total": len(active_alerts),
                "by_severity": dict(alerts_by_severity)
            },
            "recent_breakthroughs": {
                "count": len(recent_breakthroughs),
                "events": [asdict(event) for event in recent_breakthroughs[:5]]  # Latest 5
            },
            "performance_trends": performance_trends,
            "system_metrics": {
                "uptime_hours": (current_time - getattr(self, "_start_time", current_time)) / 3600,
                "total_metrics_collected": sum(len(metrics) for metrics in self.metrics.values()),
                "total_alerts_generated": len(self.alerts),
                "total_breakthroughs": len(self.breakthrough_events)
            }
        }
    
    def get_research_analytics(self) -> Dict[str, Any]:
        """Get research-focused analytics and insights."""
        
        # Paradigm performance comparison
        paradigm_performance = defaultdict(lambda: {"count": 0, "avg_time": 0, "avg_confidence": 0})
        
        for metric in list(self.metrics.get("processing_time", []))[-1000:]:
            paradigm = metric.tags.get("paradigm", "unknown")
            paradigm_performance[paradigm]["count"] += 1
            paradigm_performance[paradigm]["avg_time"] += metric.value
        
        for metric in list(self.metrics.get("confidence", []))[-1000:]:
            paradigm = metric.tags.get("paradigm", "unknown")
            if paradigm_performance[paradigm]["count"] > 0:
                paradigm_performance[paradigm]["avg_confidence"] += metric.value
        
        # Normalize averages
        for paradigm, stats in paradigm_performance.items():
            if stats["count"] > 0:
                stats["avg_time"] /= stats["count"]
                stats["avg_confidence"] /= stats["count"]
        
        # Breakthrough analysis
        breakthrough_analysis = {
            "total_breakthroughs": len(self.breakthrough_events),
            "validated_breakthroughs": sum(1 for e in self.breakthrough_events if e.validated),
            "by_paradigm": defaultdict(int),
            "by_type": defaultdict(int),
            "average_significance": 0.0
        }
        
        if self.breakthrough_events:
            total_significance = 0
            for event in self.breakthrough_events:
                breakthrough_analysis["by_paradigm"][event.paradigm] += 1
                breakthrough_analysis["by_type"][event.breakthrough_type] += 1
                total_significance += event.significance_score
            
            breakthrough_analysis["average_significance"] = total_significance / len(self.breakthrough_events)
        
        return {
            "paradigm_performance": dict(paradigm_performance),
            "breakthrough_analysis": breakthrough_analysis,
            "quality_metrics": {
                "average_confidence": self.aggregated_metrics.get("confidence", {}).get("avg", 0),
                "average_quality": self.aggregated_metrics.get("result_quality", {}).get("avg", 0),
                "statistical_significance": self.aggregated_metrics.get("statistical_significance", {}).get("avg", 0)
            },
            "research_velocity": {
                "metrics_per_hour": len(list(self.metrics.get("processing_time", []))[-3600:]),  # Rough estimate
                "breakthroughs_per_day": len([e for e in self.breakthrough_events if time.time() - e.timestamp < 24*3600])
            }
        }


# Factory function
def create_advanced_monitor(config: MonitoringConfig = None) -> AdvancedMonitor:
    """Create a new advanced monitoring system."""
    monitor = AdvancedMonitor(config)
    monitor._start_time = time.time()
    return monitor


# Global monitor instance
_global_monitor: Optional[AdvancedMonitor] = None


def get_global_monitor() -> AdvancedMonitor:
    """Get or create global monitoring instance."""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = create_advanced_monitor()
    return _global_monitor